{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e01c4ed",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceea25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import argparse\n",
    "from datetime import date, datetime, timedelta\n",
    "from cartopy.crs import NorthPolarStereo, LambertAzimuthalEqualArea\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import sklearn.metrics as skm\n",
    "#from PIL import Image\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45bd53",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844e5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "def nc_ice_comparison(start_date, end_date, path_man, path_aut, path_stats):\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        print(single_date.strftime(\"%Y-%m-%d\"))\n",
    "        day = single_date.strftime(\"%d\")\n",
    "        month = single_date.strftime(\"%m\")\n",
    "        year = single_date.strftime(\"%Y\")\n",
    "        \n",
    "        path_man_files = path_man + year + '/' + month + '/ice_conc_greenland_' + year + month + day + '*.nc'\n",
    "        path_aut_files = path_aut + year + '/' + month + '/s1_icetype_mosaic_'+ year + month + day + '0600.nc'\n",
    "    \n",
    "        aut_files = sorted(glob.glob(path_aut_files))\n",
    "        man_files = sorted(glob.glob(path_man_files))\n",
    "\n",
    "        if len(man_files) == 0:\n",
    "            print('SKIP')\n",
    "        else :\n",
    "            daily_ice_comparison(day, month, year, path_man, path_aut, path_stats)\n",
    "            \n",
    "def daily_ice_comparison(day, month, year, path_man, path_aut, path_stats):\n",
    "    \n",
    "    path_man = path_man + year + '/' + month + '/ice_conc_greenland_' + year + month + day + '*.nc'\n",
    "    path_aut = path_aut + year + '/' + month + '/s1_icetype_mosaic_'+ year + month + day + '0600.nc'\n",
    "    \n",
    "    ifiles_manu = sorted(glob.glob(path_man))\n",
    "    with Dataset(ifiles_manu[0]) as ds_man:\n",
    "        ds_man = Dataset(ifiles_manu[0])\n",
    "        x_man = ds_man['xc'][:]\n",
    "        y_man = ds_man['yc'][:]\n",
    "        grid_size = ds_man['ice_poly_id_grid'][0].shape\n",
    "        \n",
    "    with Dataset(path_aut) as ds_auto:\n",
    "        x_aut = ds_auto['xc'][:]\n",
    "        y_aut = ds_auto['yc'][:]\n",
    "        mask_aut = ds_auto['confidence'][0].filled(0) > 0\n",
    "        ice_type = ds_auto['ice_type'][0].filled(0)\n",
    "        confidence = ds_auto['confidence'][0].filled(0)\n",
    "\n",
    "    print('making mosaic')\n",
    "    mosaic, mask_mosaic = make_mosaic (ifiles_manu, grid_size)\n",
    "    \n",
    "    print('reprojecting')\n",
    "    mosaic_inter, mask_mosaic_inter = reproject(mosaic, mask_mosaic, y_man, x_man, x_aut, y_aut)\n",
    "    \n",
    "    print('ice difference')\n",
    "    man2aut, res_man, res_aut, mask_diff, mask_nan, land_mask = ice_difference (mosaic_inter, ice_type, mask_mosaic_inter, mask_aut)\n",
    "\n",
    "    intersec = np.count_nonzero(mask_diff == 1)\n",
    "    if intersec == 0 :\n",
    "        print('DATE ' + year + month + day + ' SKIPPED' )\n",
    "    else :\n",
    "        print('Statistics') \n",
    "        #data = compute_stats_all (man2aut, res_man, res_aut, mask_diff, confidence)\n",
    "        \n",
    "        print('writing data')\n",
    "        #write_stats_day(data, path_stats + 'stats_m_', year + month + day)\n",
    "        \n",
    "        print('saving images')\n",
    "        image_render(year, month, day, path_stats, man2aut, res_man, res_aut, land_mask, mask_diff)\n",
    "\n",
    "def get_man_file(path):\n",
    "    \n",
    "    with Dataset(path) as ds:\n",
    "\n",
    "        ct = ds['CT'][0]\n",
    "        ca = ds['CA'][0]\n",
    "        sa = ds['SA'][0]\n",
    "        cb = ds['CB'][0]\n",
    "        sb = ds['SB'][0]\n",
    "        cc = ds['CC'][0]\n",
    "        sc = ds['SC'][0]\n",
    "        polygon_id = ds['polygon_id'][0]\n",
    "        polygon_reference = ds['polygon_reference'][:]\n",
    "        ice_poly_id_grid = ds ['ice_poly_id_grid'][0]\n",
    "        \n",
    "    return ct,ca,sa,cb,sb,cc,sc,polygon_id, polygon_reference, ice_poly_id_grid\n",
    "\n",
    "\n",
    "\n",
    "def si_type(stage):\n",
    "    \n",
    "    index_ = 0\n",
    "    \n",
    "    if stage ==0:\n",
    "        index_ = 0\n",
    "    #print('ice_free')\n",
    "\n",
    "    if stage in range(81,86):\n",
    "        #print('Young ice')\n",
    "        index_=1\n",
    "    if stage in range(86,94):\n",
    "        #print('First year ice')\n",
    "        index_=2\n",
    "    if stage in range(95,98):\n",
    "        #print('multiyear ice')\n",
    "        index_=3\n",
    "    return index_\n",
    "\n",
    "\n",
    "\n",
    "def dominant_ice(ct,ca,sa,cb,sb,cc,sc,polygon_id, polygon_reference, ice_poly_id_grid):\n",
    "\n",
    "    dominant_grid = np.zeros(ice_poly_id_grid.shape)\n",
    "    dominant_vector = np.zeros((len(ca))).astype('int')\n",
    "    sod = [sa, sb, sc]\n",
    "    \n",
    "    for i in range (len(ca)) :\n",
    "        #dominant_vector[i] = np.argmax([ca[i], cb[i], cc[i]])\n",
    "        ice = np.argmax([ca[i], cb[i], cc[i]])\n",
    "        ice_type = si_type(sod[ice][i])\n",
    "        dominant_vector[i] = ice_type\n",
    "    \n",
    "    for p_ref in polygon_reference:\n",
    "        # ic take indices\n",
    "        ic = np.where(p_ref == polygon_reference)[0]\n",
    "        p_id = polygon_id[ic]\n",
    "        if p_id == -9:\n",
    "            continue\n",
    "        mask = ice_poly_id_grid == p_id\n",
    "        dominant_grid[mask] = dominant_vector[ic]\n",
    "        \n",
    "    return dominant_grid\n",
    "\n",
    "\n",
    "\n",
    "def make_mosaic (files, grid_size):\n",
    "    \n",
    "    # Create empty array with the size of a grid\n",
    "    mosaic = np.zeros(grid_size)\n",
    "    mask_mosaic = np.zeros(grid_size)\n",
    "    #mask_mosaic[:] = -1\n",
    "\n",
    "    # for each file in files list of the day\n",
    "    for file in files:\n",
    "        ct,ca,sa,cb,sb,cc,sc,polygon_id, polygon_reference, ice_poly_id_grid = get_man_file(file)\n",
    "        file_dominant = dominant_ice(ct,ca,sa,cb,sb,cc,sc,polygon_id, polygon_reference, ice_poly_id_grid)\n",
    "        \"\"\"mask = file_dominant >= 1 \n",
    "        may come from a previous version of the mask\"\"\"\n",
    "        \n",
    "        mask = ice_poly_id_grid.filled(-1) >= 0\n",
    "        mosaic[mask] = file_dominant[mask]\n",
    "\n",
    "        mask_mosaic[mask] = 1\n",
    "        #mask_mosaic = mask_mosaic + file_grid[0]\n",
    "        \n",
    "    return mosaic, mask_mosaic\n",
    "\n",
    "\n",
    "\n",
    "def reproject(mosaic, mask_mosaic, y_man, x_man, x_aut, y_aut):\n",
    "    \n",
    "    # Define projection of the sea ice drift product +proj=stere +lat_0=90n +lon_0=0e +lat_ts=90n +r=6371000\n",
    "    crs_aut = NorthPolarStereo(0, 90)\n",
    "    # define projection of the thickness product +proj=stere +lon_0=-45 +lat_ts=90 +lat_0=90 +a=6371000 +b=6371000\n",
    "    crs_man = NorthPolarStereo(-45, 90)\n",
    "    \n",
    "    # create matrices of coordinates for reprojection of SIT product from LAEA to NPS projection\n",
    "    # NPS coordinates on NPS grid\n",
    "    x_aut_grd, y_aut_grd = np.meshgrid(x_aut, y_aut)\n",
    "    # LAEA coordinates on NPS grid\n",
    "    grd_man = crs_man.transform_points(crs_aut, x_aut_grd, y_aut_grd)\n",
    "    x_grd_man, y_grd_man = grd_man[:,:,0], grd_man[:,:,1]\n",
    "    \n",
    "    # Prepare interpolators for thickness and concentration\n",
    "    rgi = RegularGridInterpolator((y_man, x_man), mosaic, method='nearest', bounds_error=False)\n",
    "    mask_rgi = RegularGridInterpolator((y_man, x_man), mask_mosaic, method='nearest', bounds_error=False)\n",
    "    # Do interpolation from LAEA grid onto NPS grid\n",
    "    mosaic_inter = rgi((y_grd_man, x_grd_man))\n",
    "    mask_mosaic_inter = mask_rgi((y_grd_man, x_grd_man))\n",
    "    \n",
    "    return mosaic_inter, mask_mosaic_inter\n",
    "\n",
    "\n",
    "\n",
    "def ice_difference (mosaic_inter, ice_type, mask_mosaic_inter, mask_aut):\n",
    "    \n",
    "    # Create different masks\n",
    "    mask_man = mask_mosaic_inter > 0\n",
    "    mask_common = mask_man * mask_aut\n",
    "    mask_nan = np.where(mask_common == 0, np.nan, mask_common)\n",
    "    land_mask = ice_type == -1\n",
    "    \n",
    "    # Results \n",
    "    res_man = mosaic_inter * mask_common\n",
    "    res_man = np.nan_to_num(res_man, nan=0)\n",
    "    res_aut = ice_type * mask_common\n",
    "\n",
    "    diff_man_aut = res_man - res_aut\n",
    "    \n",
    "    return diff_man_aut, res_man, res_aut, mask_common, mask_nan, land_mask\n",
    "\n",
    "\n",
    "\n",
    "def compute_stats_all (man2aut, res_man, res_aut, mask_diff, confidence):\n",
    "    \n",
    "    m_man = res_man[mask_diff]\n",
    "    m_aut = res_aut[mask_diff]\n",
    "    m_conf = confidence[mask_diff]\n",
    "    \n",
    "    # basic metric  \n",
    "    report = skm.classification_report(m_man, m_aut, digits=3, output_dict=True)\n",
    "    \n",
    "    accuracy = report['accuracy']\n",
    "    macro_avg_p = report['macro avg']['precision']\n",
    "    macro_avg_r = report['macro avg']['recall']\n",
    "    macro_avg_f = report['macro avg']['f1-score']\n",
    "    \n",
    "    weighted_avg_p = report['weighted avg']['precision']\n",
    "    weighted_avg_r = report['weighted avg']['recall']\n",
    "    weighted_avg_f = report['weighted avg']['f1-score']\n",
    "    \n",
    "    # confusion matrix   \n",
    "    matrix = skm.confusion_matrix(m_man, m_aut)\n",
    "    \n",
    "    # jaccard    possibly ok\n",
    "    jaccard_labels = skm.jaccard_score(m_man, m_aut, average=None)   # list\n",
    "    jaccard_avg = skm.jaccard_score(m_man, m_aut, average='weighted')  #float\n",
    "    \n",
    "    # Kappa    ok\n",
    "    kappa = skm.cohen_kappa_score(m_man, m_aut, labels=None, weights=None, sample_weight=None)\n",
    "    \n",
    "    # Precision recall fscore   ok                 list\n",
    "    p, r, f, s = skm.precision_recall_fscore_support(m_man, m_aut, average=None, warn_for=('precision', 'recall', 'f-score'))\n",
    "\n",
    "    # matthews_corrcoef    ok\n",
    "    mcc = skm.matthews_corrcoef(m_man, m_aut)\n",
    "    \n",
    "    # hamming_loss    possibly ok\n",
    "    hloss = skm.hamming_loss(m_man, m_aut)\n",
    "    \n",
    "    # balanced accuracy\n",
    "    b_acc = skm.balanced_accuracy_score(m_man, m_aut)\n",
    "    \n",
    "    log_loss_binary, log_loss_percentage, auc_roc_binary, auc_roc_percentage = confidence_metrics(m_man, m_aut, m_conf)\n",
    "    \n",
    "    # Count px in comparison, manual and automatic images\n",
    "\n",
    "    total_man = []\n",
    "    total_aut = []\n",
    "    total = []\n",
    "    ind = []\n",
    "\n",
    "    for i in range (-3,4):\n",
    "        count = np.count_nonzero(man2aut[mask_diff] == i)\n",
    "        total.append(count)\n",
    "        ind.append(i)\n",
    "\n",
    "    for i in range (4):\n",
    "        count_man = np.count_nonzero(res_man[mask_diff] == i)\n",
    "        count_aut = np.count_nonzero(res_aut[mask_diff] == i)\n",
    "        total_man.append(count_man)\n",
    "        total_aut.append(count_aut)\n",
    "    \n",
    "    report_avg = [accuracy, macro_avg_p, macro_avg_r, macro_avg_f, weighted_avg_p, weighted_avg_r, weighted_avg_f]\n",
    "    conf_indexes = [log_loss_binary, log_loss_percentage, auc_roc_binary, auc_roc_percentage]\n",
    "    mat_lis = [p, r, f, s, matrix, jaccard_labels, total, total_man, total_aut] \n",
    "    indexes = [b_acc, hloss, mcc, kappa, jaccard_avg]\n",
    "    \n",
    "    datas = report_avg + conf_indexes + mat_lis + indexes\n",
    "    \n",
    "    return datas\n",
    "\n",
    "\n",
    "\n",
    "def confidence_metrics(m_man, m_aut, m_conf):\n",
    "    \n",
    "    binary = []\n",
    "    percentage = []\n",
    "\n",
    "    for i in range (len(m_man)):\n",
    "        proba = [0,0,0,0]\n",
    "        proba[m_aut[i]] = 1\n",
    "        binary.append(proba)\n",
    "\n",
    "        max_conf = m_conf[i]*0.01\n",
    "        min_conf = (1-max_conf)/3\n",
    "\n",
    "        proba_ = [min_conf,min_conf,min_conf,min_conf]\n",
    "        proba_[m_aut[i]] = max_conf\n",
    "        percentage.append(proba_)\n",
    "\n",
    "    log_loss_binary = skm.log_loss(m_man, binary,  labels=np.array([0. ,1. ,2. ,3.]))\n",
    "    log_loss_percentage = skm.log_loss(m_man, percentage, labels=np.array([0. ,1. ,2. ,3.]))\n",
    "\n",
    "    try :\n",
    "        auc_roc_binary = skm.roc_auc_score(m_man, binary, multi_class='ovr')\n",
    "    except ValueError:\n",
    "        auc_roc_binary = 0.0\n",
    "    try:\n",
    "        auc_roc_percentage = skm.roc_auc_score(m_man, percentage, multi_class='ovr')\n",
    "    except ValueError:\n",
    "        auc_roc_percentage = 0.0\n",
    "    \n",
    "    return log_loss_binary, log_loss_percentage, auc_roc_binary, auc_roc_percentage\n",
    "\n",
    "\n",
    "\n",
    "def write_stats_day(datas, path_stats, filename):\n",
    "    \n",
    "    with open(path_stats + filename + '.txt', \"a\") as file:\n",
    "        \n",
    "        for data in datas:\n",
    "    \n",
    "            #print(type(data))\n",
    "    \n",
    "            if type(data) == float or isinstance(data, np.float64):\n",
    "                file.write(str(data) + \"\\n\")\n",
    "\n",
    "            if type(data) == list:\n",
    "                count = ';'.join(map(str, data))\n",
    "                file.write(count+\"\\n\")\n",
    "\n",
    "            if isinstance(data, np.ndarray):\n",
    "                if data.ndim == 1:\n",
    "                    count = ';'.join(map(str, data))\n",
    "                    file.write(count+\"\\n\")\n",
    "\n",
    "            if isinstance(data, np.ndarray):\n",
    "                if data.ndim == 2:\n",
    "                    file.write(\"Confusion matrix\\n\")\n",
    "                    rows = [\"{};{};{};{}\".format(i, j, k, l) for i, j, k, l in data]\n",
    "                    conf = \"\\n\".join(rows)\n",
    "                    file.write(conf)\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def image_render(year, month, day, path_img, man2aut, res_man, res_aut, land_mask, mask_diff):\n",
    "\n",
    "    # adapt array with values for no data and land\n",
    "\n",
    "    # difference between manual and automatic\n",
    "    img = man2aut\n",
    "    img[~mask_diff] = -4\n",
    "    img[land_mask] = -5\n",
    "\n",
    "    # manual\n",
    "    img_man = res_man\n",
    "    img_man[~mask_diff] = -1\n",
    "    img_man[land_mask] = -2\n",
    "\n",
    "    # automatic\n",
    "    img_aut = res_aut\n",
    "    img_aut[~mask_diff] = -1\n",
    "    img_aut[land_mask] = -2\n",
    "\n",
    "\n",
    "    # Colormap for comparison\n",
    "    cmap = plt.cm.colors.ListedColormap(['gray','white' ,\n",
    "                                         '#b30727', '#e8775d', '#f0cab7', '#cfd9e8', '#b5cdf8', '#6485ec', '#384abf'])\n",
    "    # Colormap for ice type (from H.Boulze)\n",
    "    cmap_hugo = plt.cm.colors.ListedColormap(['whitesmoke', 'white', '#0064ff', '#aa28f0', '#ffff00', '#b46432'])\n",
    "\n",
    "    # Normalization of ice comparison\n",
    "    norm = plt.Normalize(-5, 4)\n",
    "    img_norm = norm(img)\n",
    "    img_ = cmap(img_norm)\n",
    "\n",
    "    # Normalization of ice types\n",
    "    norm2 = plt.Normalize(-2,4)\n",
    "    img_man_norm = norm2(img_man)\n",
    "    img_man = cmap_hugo(img_man_norm)\n",
    "    img_aut_norm = norm2(img_aut)\n",
    "    img_aut = cmap_hugo(img_aut_norm)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    gs = GridSpec(3, 3, width_ratios=[4, 3, 3], height_ratios=[3, 3, 3])\n",
    "    fig.suptitle(\"Ice comparison \" + year + \"-\" + month + \"-\" + day, fontsize='x-large')\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    im1 = ax1.imshow(img_, cmap=cmap, aspect='auto')\n",
    "    ax1.set_title('Comparison')\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    im2 = ax2.imshow(img_man, cmap=cmap_hugo, aspect='auto')\n",
    "    ax2.set_title('Manual classification')\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    im3 = ax3.imshow(img_aut, cmap=cmap_hugo, aspect='auto')\n",
    "    ax3.set_title('Automatic classification')\n",
    "\n",
    "    cbar_comp = plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "    cbar_comp.ax.get_yaxis().set_ticks([])\n",
    "    for j, lab in enumerate(['ground','no data','-3','-2', '-1', '0', '1', '2', '3']):\n",
    "        cbar_comp.ax.text(1.3, (j + 0.5) / 9.0, lab, ha='left', va='center', fontsize='small')\n",
    "\n",
    "    cbaxes = fig.add_axes([0.5, 0.62, 0.4, 0.02])\n",
    "    cbar = plt.colorbar(im2, ax=[ax2, ax3], orientation='horizontal', cax = cbaxes)\n",
    "\n",
    "    cbar.ax.get_xaxis().set_ticks([])\n",
    "    for j, lab in enumerate(['Ground','No Data','Ice free','Young Ice', 'First Year Ice', 'Multi Year Ice']):\n",
    "        cbar.ax.text((j + 0.5) / 6.0, .5, lab, ha='center', va='center', fontsize='small')\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(path_img+\"map_\"+year+month+day+\".png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b976b3e",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9935de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#day = '30'\n",
    "#month = '01'\n",
    "#start_date = date(2023, 1, 1)\n",
    "#end_date = date(2023, 2, 5)\n",
    "\n",
    "#path_aut = '/Data/sat/auxdata/ice_charts/NERSC/nrt.cmems-du.eu/Core/SEAICE_ARC_PHY_AUTO_L4_NRT_011_015/cmems_obs-si_arc_phy-icetype_nrt_L4-auto_P1D/'\n",
    "#path_man = '/Data/sat/auxdata/ice_charts/DMI/nrt.cmems-du.eu/Core/SEAICE_ARC_SEAICE_L4_NRT_OBSERVATIONS_011_002/cmems_obs-si_arc_physic_nrt_1km-grl_P1D-irr/'\n",
    "#path_stats = '/home/malela/data/comp2/'\n",
    "#path_stats = '/home/malela/data/img/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e66ab421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] start end path_man path_aut path_stats\n",
      "ipykernel_launcher.py: error: the following arguments are required: end, path_man, path_aut, path_stats\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#nc_ice_comparison(start_date, end_date, path_man, path_aut, path_stats)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"start\", help=\"start date of computation YYYY-mm-dd\")\n",
    "parser.add_argument(\"end\", help=\"end date of computation YYYY-mm-dd\")\n",
    "parser.add_argument(\"path_man\", help=\"path of manuals data /path/to/data/\")\n",
    "parser.add_argument(\"path_aut\", help=\"path of automatics data /path/to/data/\")\n",
    "parser.add_argument(\"path_stats\", help=\"path of statistics results and images /path/to/data/\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "istart = args.start\n",
    "istart = istart.split('-')\n",
    "start_date = date(int(istart[0]), int(istart[1]), int(istart[2]))\n",
    "iend = args.end\n",
    "iend = iend.split('-')\n",
    "end_date = date(int(iend[0]), int(iend[1]), int(iend[2]))\n",
    "\n",
    "nc_ice_comparison(start_date, end_date, args.path_man, args.path_aut, args.path_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831b6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
