{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bbd460e",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6664087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartopy.crs import NorthPolarStereo, LambertAzimuthalEqualArea, Globe\n",
    "import cartopy.feature as cfeature\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from datetime import date, datetime, timedelta\n",
    "import glob\n",
    "import sklearn.metrics as skm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "from osgeo import gdal, osr, ogr\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206d853",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68a6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdal_dataset(x_ul, nx, dx, y_ul, ny, dy, srs_proj4, dtype=gdal.GDT_Float32):\n",
    "    \"\"\"\n",
    "    Get empty gdal dataset with a given extent and projection\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_ul : float\n",
    "        x coordinates of upper-left corner of upper-left pixel\n",
    "        ([0,0] pixel)\n",
    "    nx   : int\n",
    "        number of pixels in x-direction (number of columns)\n",
    "    dx   : float\n",
    "        step size in x direction (as column index increases)\n",
    "        (can be negative)\n",
    "    y_ul : float\n",
    "        y coordinates of upper-left corner of upper-left pixel\n",
    "        ([0,0] pixel)\n",
    "    ny   : int\n",
    "        number of pixels in y-direction (number of rows)\n",
    "    dy   : float\n",
    "        step size in y direction (as row index increases)\n",
    "        (can be negative)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    ds : osgeo.gdal.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # create dataset\n",
    "    dst_ds = gdal.GetDriverByName('MEM').Create('tmp', nx, ny, 1, dtype)\n",
    "\n",
    "    # set grid limits\n",
    "    # - for usage of osgeo.gdal.Dataset.SetGeoTransform, see:\n",
    "    #   https://gdal.org/tutorials/geotransforms_tut.html\n",
    "    dst_ds.SetGeoTransform((x_ul, dx, 0, y_ul, 0, dy))\n",
    "\n",
    "    # set projection\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromProj4(str(srs_proj4))\n",
    "    srs_wkt = srs.ExportToWkt()\n",
    "    dst_ds.SetProjection(srs_wkt)\n",
    "\n",
    "    # set no_data_value for the band\n",
    "    band = dst_ds.GetRasterBand(1)\n",
    "    NoData_value = -999999\n",
    "    band.SetNoDataValue(NoData_value)\n",
    "    band.FlushCache()\n",
    "\n",
    "    return dst_ds\n",
    "\n",
    "def rasterize_icehart(shapefile, ds):\n",
    "    field_names = ['CT', 'CA', 'CB', 'CC', 'SA', 'SB', 'SC', 'FA', 'FB', 'FC']\n",
    "    field_arr = []\n",
    "\n",
    "    ivector = ogr.Open(shapefile, 0)\n",
    "    ilayer = ivector.GetLayer()\n",
    "\n",
    "    odriver = ogr.GetDriverByName('MEMORY')\n",
    "    ovector = odriver.CreateDataSource('memData')\n",
    "    olayer = ovector.CopyLayer(ilayer, 'burn_ice_layer', ['OVERWRITE=YES'])\n",
    "    fidef = ogr.FieldDefn('poly_index', ogr.OFTInteger)\n",
    "    olayer.CreateField(fidef)\n",
    "    for ft in olayer:\n",
    "        ft_id = ft.GetFID() + 1\n",
    "        field_vec = [ft_id] + [float(ft.GetField(field_name)) for field_name in field_names]\n",
    "        field_arr.append(field_vec)\n",
    "        ft.SetField('poly_index', ft_id)\n",
    "        olayer.SetFeature(ft)\n",
    "\n",
    "    gdal.RasterizeLayer(ds, [1], olayer, options=[\"ATTRIBUTE=poly_index\"])\n",
    "    dst_arr = ds.ReadAsArray()\n",
    "    return dst_arr, np.array(field_arr)\n",
    "\n",
    "\n",
    "def SI_type(stage):\n",
    "    \n",
    "    index_ = 0\n",
    "    \n",
    "    if stage ==0:\n",
    "        index_ = 0\n",
    "    #print('ice_free')\n",
    "\n",
    "    if stage in range(81,86):\n",
    "        #print('Young ice')\n",
    "        index_=1\n",
    "    if stage in range(86,94):\n",
    "        #print('First year ice')\n",
    "        index_=2\n",
    "    if stage in range(95,98):\n",
    "        #print('multiyear ice')\n",
    "        index_=3\n",
    "    return index_\n",
    "\n",
    "\n",
    "def ice_type_map(polyindex_arr, icecodes):    \n",
    "    it_array = np.zeros(polyindex_arr.shape, float)\n",
    "    it_array[:] = -1\n",
    "\n",
    "    polyids = np.unique(polyindex_arr)\n",
    "\n",
    "\n",
    "    for polyid in polyids:\n",
    "        mask = polyindex_arr == polyid\n",
    "        i = np.where(icecodes[:,0] == polyid)[0]\n",
    "        if i:\n",
    "            #if icecodes[i,1] >= 10 :\n",
    "            ice = np.argmax([icecodes[i,2], icecodes[i,3], icecodes[i,4]])\n",
    "            sod = [icecodes[i,5], icecodes[i,6], icecodes[i,7]]\n",
    "            ice_type = SI_type(sod[ice])\n",
    "\n",
    "            #ct = icecodes[i,1]\n",
    "            it_array[mask] = ice_type\n",
    "    return it_array\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def weekrange(end_date):\n",
    "    for n in range(6):\n",
    "        yield end_date - timedelta(n)\n",
    "\n",
    "\n",
    "def week_auto_files(str_date):\n",
    "    d = str_date.split('-')\n",
    "    end_date = date(int(d[0]), int(d[1]), int(d[2]))\n",
    "    aut_files = []\n",
    "\n",
    "    for single_date in weekrange(end_date):\n",
    "        print(single_date.strftime(\"%Y-%m-%d\"))\n",
    "        day = single_date.strftime(\"%d\")\n",
    "        month = single_date.strftime(\"%m\")\n",
    "        year = single_date.strftime(\"%Y\")\n",
    "\n",
    "        aut_file = sorted(glob.glob(path_aut + year + '/' + month + '/s1_icetype_mosaic_'+year+month+day+'0600.nc')) \n",
    "        aut_files.append(aut_file[0])\n",
    "        \n",
    "    return aut_files\n",
    "\n",
    "\n",
    "def mosaic_auto_argmax(aut_files):\n",
    "    with Dataset(aut_files[5]) as ds:\n",
    "        n, m = ds ['ice_type'][0].filled(0).shape\n",
    "\n",
    "    maps = []\n",
    "    prob = []\n",
    "\n",
    "    for file in aut_files:\n",
    "        with Dataset(file) as ds:\n",
    "            ice_type = ds['ice_type'][0].filled(4)\n",
    "            confidence = ds['confidence'][0]\n",
    "            ice_type[confidence == 0] = 4\n",
    "            ice_type[ice_type == -1] = 4\n",
    "\n",
    "        maps.append(ice_type)\n",
    "        prob.append(confidence)\n",
    "        \n",
    "    cols, rows = np.meshgrid(range(m), range(n))\n",
    "    sum_prob = np.zeros((n,m,4))\n",
    "\n",
    "    for p, m in zip(prob, maps):\n",
    "        gpi = m < 4\n",
    "        sum_prob[rows[gpi], cols[gpi], m[gpi]] += p[gpi]\n",
    "        \n",
    "    max_prob_idx = np.argmax(sum_prob, axis=2)\n",
    "\n",
    "    max_prob_idx[sum_prob.sum(axis=2) == 0] = -1\n",
    "    \n",
    "    return max_prob_idx\n",
    "\n",
    "\n",
    "def is_difference (mosaic_aut, usice_inter):\n",
    "    mask_aut = mosaic_aut >= 0\n",
    "    mask_usnic = usice_inter >= 0\n",
    "    mask_common = mask_aut * mask_usnic\n",
    "\n",
    "    res_aut = mosaic_aut * mask_common\n",
    "    res_usnic = usice_inter * mask_common\n",
    "    res_usnic = np.nan_to_num(res_usnic, nan=0)\n",
    "\n",
    "    diff_us_aut = res_usnic - res_aut\n",
    "    return diff_us_aut, res_aut, res_usnic, mask_common\n",
    "\n",
    "\n",
    "def compute_stats_us_aut (man2aut, res_man, res_aut, mask_diff):\n",
    "    \n",
    "    m_man = res_man[mask_diff]\n",
    "    m_aut = res_aut[mask_diff]\n",
    "    \n",
    "    # basic metric  \n",
    "    report = skm.classification_report(m_man, m_aut, digits=3, output_dict=True)\n",
    "    \n",
    "    accuracy = report['accuracy']\n",
    "    macro_avg_p = report['macro avg']['precision']\n",
    "    macro_avg_r = report['macro avg']['recall']\n",
    "    macro_avg_f = report['macro avg']['f1-score']\n",
    "    \n",
    "    weighted_avg_p = report['weighted avg']['precision']\n",
    "    weighted_avg_r = report['weighted avg']['recall']\n",
    "    weighted_avg_f = report['weighted avg']['f1-score']\n",
    "    \n",
    "    # confusion matrix   \n",
    "    matrix = skm.confusion_matrix(m_man, m_aut)\n",
    "    \n",
    "    # jaccard    possibly ok\n",
    "    jaccard_labels = skm.jaccard_score(m_man, m_aut, average=None)   # list\n",
    "    jaccard_avg = skm.jaccard_score(m_man, m_aut, average='weighted')  #float\n",
    "    \n",
    "    # Kappa    ok\n",
    "    kappa = skm.cohen_kappa_score(m_man, m_aut, labels=None, weights=None, sample_weight=None)\n",
    "    \n",
    "    # Precision recall fscore   ok                 list\n",
    "    p, r, f, s = skm.precision_recall_fscore_support(m_man, m_aut, average=None, warn_for=('precision', 'recall', 'f-score'))\n",
    "\n",
    "    # matthews_corrcoef    ok\n",
    "    mcc = skm.matthews_corrcoef(m_man, m_aut)\n",
    "    \n",
    "    # hamming_loss    possibly ok\n",
    "    hloss = skm.hamming_loss(m_man, m_aut)\n",
    "    \n",
    "    # balanced accuracy\n",
    "    b_acc = skm.balanced_accuracy_score(m_man, m_aut)\n",
    "    \n",
    "    # Count px in comparison, manual and automatic images\n",
    "\n",
    "    total_man = []\n",
    "    total_aut = []\n",
    "    total = []\n",
    "    ind = []\n",
    "\n",
    "    for i in range (-3,4):\n",
    "        count = np.count_nonzero(man2aut[mask_diff] == i)\n",
    "        total.append(count)\n",
    "        ind.append(i)\n",
    "\n",
    "    for i in range (4):\n",
    "        count_man = np.count_nonzero(res_man[mask_diff] == i)\n",
    "        count_aut = np.count_nonzero(res_aut[mask_diff] == i)\n",
    "        total_man.append(count_man)\n",
    "        total_aut.append(count_aut)\n",
    "    \n",
    "    report_avg = [accuracy, macro_avg_p, macro_avg_r, macro_avg_f, weighted_avg_p, weighted_avg_r, weighted_avg_f]\n",
    "    mat_lis = [p, r, f, s, matrix, jaccard_labels, total, total_man, total_aut] \n",
    "    indexes = [b_acc, hloss, mcc, kappa, jaccard_avg]\n",
    "    \n",
    "    datas = report_avg + mat_lis + indexes\n",
    "    \n",
    "    return datas\n",
    "\n",
    "\n",
    "def write_stats_day(datas, path_stats, filename):\n",
    "    \n",
    "    with open(path_stats + filename + '.txt', \"a\") as file:\n",
    "        \n",
    "        for data in datas:\n",
    "    \n",
    "            #print(type(data))\n",
    "    \n",
    "            if type(data) == float or isinstance(data, np.float64):\n",
    "                file.write(str(data) + \"\\n\")\n",
    "\n",
    "            if type(data) == list:\n",
    "                count = ';'.join(map(str, data))\n",
    "                file.write(count+\"\\n\")\n",
    "\n",
    "            if isinstance(data, np.ndarray):\n",
    "                if data.ndim == 1:\n",
    "                    count = ';'.join(map(str, data))\n",
    "                    file.write(count+\"\\n\")\n",
    "\n",
    "            if isinstance(data, np.ndarray):\n",
    "                if data.ndim == 2:\n",
    "                    file.write(\"Confusion matrix\\n\")\n",
    "                    rows = [\"{};{};{};{}\".format(i, j, k, l) for i, j, k, l in data]\n",
    "                    conf = \"\\n\".join(rows)\n",
    "                    file.write(conf)\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def image_render(year, month, day, path_img, man2aut, res_man, res_aut, land_mask, mask_diff):\n",
    "\n",
    "    # adapt array with values for no data and land\n",
    "\n",
    "    # difference between manual and automatic\n",
    "    img = man2aut\n",
    "    img[~mask_diff] = -4\n",
    "    img[land_mask] = -5\n",
    "\n",
    "    # manual\n",
    "    img_man = res_man\n",
    "    img_man[~mask_diff] = -1\n",
    "    img_man[land_mask] = -2\n",
    "\n",
    "    # automatic\n",
    "    img_aut = res_aut\n",
    "    img_aut[~mask_diff] = -1\n",
    "    img_aut[land_mask] = -2\n",
    "\n",
    "\n",
    "    # Colormap for comparison\n",
    "    cmap = plt.cm.colors.ListedColormap(['gray','white' ,\n",
    "                                         '#b30727', '#e8775d', '#f0cab7', '#cfd9e8', '#b5cdf8', '#6485ec', '#384abf'])\n",
    "    # Colormap for ice type (from H.Boulze)\n",
    "    cmap_hugo = plt.cm.colors.ListedColormap(['whitesmoke', 'white', '#0064ff', '#aa28f0', '#ffff00', '#b46432'])\n",
    "\n",
    "    # Normalization of ice comparison\n",
    "    norm = plt.Normalize(-5, 4)\n",
    "    img_norm = norm(img)\n",
    "    img_ = cmap(img_norm)\n",
    "\n",
    "    # Normalization of ice types\n",
    "    norm2 = plt.Normalize(-2,4)\n",
    "    img_man_norm = norm2(img_man)\n",
    "    img_man = cmap_hugo(img_man_norm)\n",
    "    img_aut_norm = norm2(img_aut)\n",
    "    img_aut = cmap_hugo(img_aut_norm)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    gs = GridSpec(3, 3, width_ratios=[4, 3, 3], height_ratios=[3, 3, 3])\n",
    "    fig.suptitle(\"Ice comparison \" + year + \"-\" + month + \"-\" + day, fontsize='x-large')\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    im1 = ax1.imshow(img_, cmap=cmap, aspect='auto')\n",
    "    ax1.set_title('Comparison')\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    im2 = ax2.imshow(img_man, cmap=cmap_hugo, aspect='auto')\n",
    "    ax2.set_title('Manual classification')\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    im3 = ax3.imshow(img_aut, cmap=cmap_hugo, aspect='auto')\n",
    "    ax3.set_title('Automatic classification')\n",
    "\n",
    "    cbar_comp = plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "    cbar_comp.ax.get_yaxis().set_ticks([])\n",
    "    for j, lab in enumerate(['ground','no data','-3','-2', '-1', '0', '1', '2', '3']):\n",
    "        cbar_comp.ax.text(1.3, (j + 0.5) / 9.0, lab, ha='left', va='center', fontsize='small')\n",
    "\n",
    "    cbaxes = fig.add_axes([0.5, 0.62, 0.4, 0.02])\n",
    "    cbar = plt.colorbar(im2, ax=[ax2, ax3], orientation='horizontal', cax = cbaxes)\n",
    "\n",
    "    cbar.ax.get_xaxis().set_ticks([])\n",
    "    for j, lab in enumerate(['Ground','No Data','Ice free','Young Ice', 'First Year Ice', 'Multi Year Ice']):\n",
    "        cbar.ax.text((j + 0.5) / 6.0, .5, lab, ha='center', va='center', fontsize='small')\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(path_img+\"map_\"+year+month+day+\".png\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "def nic_nersc_day(year, month, day, path_nic, path_nersc, path_stats):\n",
    "\n",
    "    shapefile = path_nic + 'ARCTIC' + year[2:4] + month + day + '.shp'\n",
    "\n",
    "    x_ul = -871516.0\n",
    "    nx = 2800\n",
    "    dx = 1000.0\n",
    "    y_ul = 57017.0\n",
    "    ny = 2500\n",
    "    dy = -1000.0\n",
    "    srs = '+proj=stere +lat_0=90.0 +lon_0=0.0 +lat_ts=90.0 +R=6.371e+06 +units=m +no_defs'\n",
    "\n",
    "    # Rasterize the manual ice chart\n",
    "    ds = get_gdal_dataset(x_ul, nx, dx, y_ul, ny, dy, srs, gdal.GDT_Int16)\n",
    "    polyindex_arr, icecodes = rasterize_icehart(shapefile, ds)\n",
    "    map_ice = ice_type_map(polyindex_arr, icecodes)\n",
    "\n",
    "    # Mosaic the automatics files\n",
    "    aut_files = week_auto_files('2023-' + month + '-' + day)\n",
    "    auto_mosaic = mosaic_auto_argmax(aut_files)\n",
    "\n",
    "    with Dataset(aut_files[0]) as ds:\n",
    "        land_mask = ds['ice_type'][0].filled(0) == -1\n",
    "\n",
    "    diff_us_aut, res_aut, res_usnic, mask_common = is_difference (auto_mosaic, map_ice)\n",
    "\n",
    "    data = compute_stats_us_aut (diff_us_aut, res_usnic, res_aut, mask_common)\n",
    "\n",
    "    filename = 'us_stats_23' + month + day\n",
    "\n",
    "    write_stats_day(data, path_stats, filename)\n",
    "\n",
    "    image_render('2023', month, day, path_stats, diff_us_aut, res_usnic, res_aut, land_mask, mask_common)\n",
    "    \n",
    "\n",
    "def nic_nersc_comparison(start_date, end_date, path_nic, path_nersc, path_stats):\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        #print(single_date.strftime(\"%Y-%m-%d\"))\n",
    "        day = single_date.strftime(\"%d\")\n",
    "        month = single_date.strftime(\"%m\")\n",
    "        year = single_date.strftime(\"%Y\")\n",
    "        \n",
    "        man_file = sorted(glob.glob(path_nic + 'ARCTIC' + year[2:4] + month + day + '.shp'))\n",
    "        if len(man_file) == 1:\n",
    "            nic_nersc_day(year, month, day, path_nic, path_nersc, path_stats)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347cc411",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d667d52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] start end path_man path_aut path_stats\n",
      "ipykernel_launcher.py: error: the following arguments are required: end, path_man, path_aut, path_stats\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"start\", help=\"start date of computation YYYY-mm-dd\")\n",
    "parser.add_argument(\"end\", help=\"end date of computation YYYY-mm-dd\")\n",
    "parser.add_argument(\"path_man\", help=\"path of manuals data /path/to/data/\")\n",
    "parser.add_argument(\"path_aut\", help=\"path of automatics data /path/to/data/\")\n",
    "parser.add_argument(\"path_stats\", help=\"path of statistics results and images /path/to/data/\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "istart = args.start\n",
    "istart = istart.split('-')\n",
    "start_date = date(int(istart[0]), int(istart[1]), int(istart[2]))\n",
    "iend = args.end\n",
    "iend = iend.split('-')\n",
    "end_date = date(int(iend[0]), int(iend[1]), int(iend[2]))\n",
    "\n",
    "nic_nersc_comparison(start_date, end_date, args.path_man, args.path_aut, args.path_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e649e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
